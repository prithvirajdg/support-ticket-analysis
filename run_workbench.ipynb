{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Support Ticket Clustering Pipeline\n\nThis notebook runs the full clustering pipeline on Vertex AI Workbench.\nEverything runs from within this notebook — no terminal needed.\n\n**Data stays within GCP** — the input CSV is pulled from a GCS bucket,\nand results are pushed back to GCS. Raw transcripts never leave the cloud.\n\n**Steps:**\n1. Configure settings (project, bucket, input path)\n2. Install dependencies\n3. Upload code files + pull input CSV from GCS\n4. Run disaggregation (Gemini API)\n5. Run embedding\n6. Run clustering\n7. Preview results\n8. Push results to GCS\n\n**Estimated time:** ~30-45 min for 5K transcripts"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "**Edit these values before running:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION - EDIT THESE VALUES\n# =============================================================================\n\n# Your GCP project ID\nPROJECT_ID = \"your-project-id\"  # <- Change this!\n\n# GCS bucket name (without gs://)\nBUCKET = \"your-bucket-name\"  # <- Change this!\n\n# Input CSV path in GCS — raw data stays within GCP, never on a local machine\n# (see step 3 below for how to upload your CSV to this location)\nGCS_INPUT = \"gs://your-bucket-name/inputs/tickets.csv\"  # <- Change this!\n\n# Local filename once pulled to the Workbench VM (no need to change)\nINPUT_FILE = \"input.csv\"\n\n# Processing options\nLIMIT = 5000  # Number of transcripts to process (0 = all)\nWORKERS = 20  # Concurrent API calls\n\n# Clustering options\nMIN_CLUSTER_SIZE = 15  # Minimum items to form a cluster\nPROB_THRESHOLD = 0.0   # Minimum HDBSCAN membership probability (0.0 = keep all, try 0.05 to push uncertain points to noise)\n\n# Vertex AI region for Gemini\nREGION = \"us-central1\"  # Gemini is widely available across regions\n\n# =============================================================================\n# DON'T EDIT BELOW THIS LINE\n# =============================================================================\n\nGCS_OUTPUT = f\"gs://{BUCKET}/outputs\"\n\nprint(\"Configuration:\")\nprint(f\"  Project:          {PROJECT_ID}\")\nprint(f\"  Input (GCS):      {GCS_INPUT}\")\nprint(f\"  Output (GCS):     {GCS_OUTPUT}\")\nprint(f\"  Limit:            {LIMIT} transcripts\")\nprint(f\"  Workers:          {WORKERS} concurrent calls\")\nprint(f\"  Min cluster size: {MIN_CLUSTER_SIZE}\")\nprint(f\"  Prob threshold:   {PROB_THRESHOLD}\")\nprint(f\"  Region:           {REGION}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "Run this once per Workbench session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q google-cloud-aiplatform sentence-transformers umap-learn hdbscan pyarrow\nprint(\"Dependencies installed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Set Up Files\n\n### 3a. Upload pipeline code via JupyterLab file browser\n\nUpload the following **code files only** using the file browser (left sidebar, upload button):\n- `disaggregate.py`\n- `embed.py`\n- `cluster.py`\n- `pipeline_utils.py`\n- `system_prompt.txt`\n- `examples.txt`\n\nThese are small, non-sensitive code files.\n\n### 3b. Upload your input CSV to GCS\n\nYour raw support transcript CSV contains sensitive data and should **never be downloaded to a local machine**. Upload it directly to GCS from wherever it lives:\n\n**Option 1 — GCP Console (browser):**\n1. Go to **GCP Console -> Cloud Storage -> Buckets**\n2. Open your bucket (or create one)\n3. Create an `inputs/` folder\n4. Click **Upload Files** and select your CSV\n\n**Option 2 — From a GCP VM or Cloud Shell that already has the data:**\n```bash\ngsutil cp /path/to/your/tickets.csv gs://your-bucket-name/inputs/tickets.csv\n```\n\n**Option 3 — From this Workbench notebook (if the CSV is already on another GCP service):**\nRun this in a cell:\n```python\n!gsutil cp gs://source-bucket/path/to/tickets.csv gs://your-bucket-name/inputs/tickets.csv\n```\n\nThen run the cells below to pull the CSV to the VM and verify everything."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport subprocess\n\n# --- Pull input CSV from GCS to the Workbench VM ---\nprint(f\"Pulling input CSV from GCS...\")\nprint(f\"  Source:      {GCS_INPUT}\")\nprint(f\"  Destination: {INPUT_FILE}\")\nresult = subprocess.run(\n    [\"gsutil\", \"cp\", GCS_INPUT, INPUT_FILE],\n    capture_output=True, text=True\n)\nif result.returncode != 0:\n    print(f\"\\nERROR pulling from GCS:\\n{result.stderr}\")\n    print(\"\\nCheck that:\")\n    print(f\"  1. The file exists at {GCS_INPUT}\")\n    print(f\"  2. The Workbench service account has read access to the bucket\")\nelse:\n    size = os.path.getsize(INPUT_FILE)\n    print(f\"  Downloaded: {size:,} bytes\")\n\n# --- Verify all required files are present ---\nprint(\"\\n--- File check ---\")\ncode_files = [\n    \"disaggregate.py\",\n    \"embed.py\",\n    \"cluster.py\",\n    \"pipeline_utils.py\",\n    \"system_prompt.txt\",\n    \"examples.txt\",\n]\nall_files = code_files + [INPUT_FILE]\nmissing = [f for f in all_files if not os.path.exists(f)]\n\nif missing:\n    print(\"MISSING FILES:\")\n    for f in missing:\n        if f == INPUT_FILE:\n            print(f\"  - {f}  (failed to pull from GCS — see error above)\")\n        else:\n            print(f\"  - {f}  (upload via JupyterLab file browser)\")\nelse:\n    print(\"All files present:\")\n    for f in all_files:\n        size = os.path.getsize(f)\n        label = \"(from GCS)\" if f == INPUT_FILE else \"(uploaded)\"\n        print(f\"  {f} ({size:,} bytes) {label}\")\n    print(\"\\nReady to run the pipeline!\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Alternative: Run in Terminal (Disconnect-Safe)\n\n**Only use this if your job will take >30 minutes** (e.g. processing 20K+ transcripts).\n\nThe notebook cells above work fine for smaller jobs. But if your browser disconnects\nduring a long run, the job stops. The terminal approach uses `nohup` so the job\nsurvives disconnects.\n\n**This is an ALTERNATIVE to running cells 4-6 above.** Choose one:\n- **Notebook cells:** Simpler, see output live, but requires browser to stay connected\n- **Terminal:** Disconnect-safe, check logs later, better for long jobs",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**Terminal commands** — open **File -> New -> Terminal** in JupyterLab, then copy-paste:\n\n```bash\n# Run full pipeline with nohup (disconnect-safe)\n# Replace YOUR_PROJECT_ID and your-tickets.csv with your actual values\nnohup bash -c '\n    python3 disaggregate.py --input your-tickets.csv --output disaggregated.parquet \\\n        --project YOUR_PROJECT_ID --region us-central1 --workers 20 --limit 0 && \\\n    python3 embed.py disaggregated.parquet embedded.parquet && \\\n    python3 cluster.py embedded.parquet clustered.csv --min-cluster-size 15 && \\\n    echo \"DONE at $(date)\"\n' > pipeline.log 2>&1 &\n\n# Check if it's running\nps aux | grep python3\n\n# Watch progress (Ctrl+C to stop watching, job continues)\ntail -f pipeline.log\n```\n\n**To check on job later:**\n```bash\ntail -50 pipeline.log          # see recent output\nps aux | grep python3           # check if still running\ngrep \"DONE\" pipeline.log        # check if finished\n```\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Run Disaggregation (Gemini API)\n\nThis calls Gemini to analyze each transcript. Takes ~15-30 min for 5K.\n\n**Features:**\n- JSON mode (`response_mime_type`) for reliable structured output\n- Checkpoint saved after every batch (50 rows)\n- If job fails, run the next cell to resume\n\n**Cost:** ~$2.50 for 5K transcripts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python3 disaggregate.py \\\n    --input {INPUT_FILE} \\\n    --output disaggregated.parquet \\\n    --project {PROJECT_ID} \\\n    --region {REGION} \\\n    --workers {WORKERS} \\\n    --limit {LIMIT}"
  },
  {
   "cell_type": "markdown",
   "source": "### 4b. Resume from Checkpoint (if job failed)\n\nOnly run this if the cell above failed partway through:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Resume from last checkpoint (only run if step 4 failed)\n!python3 disaggregate.py \\\n    --input {INPUT_FILE} \\\n    --output disaggregated.parquet \\\n    --project {PROJECT_ID} \\\n    --region {REGION} \\\n    --workers {WORKERS} \\\n    --limit {LIMIT} \\\n    --resume",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Embedding\n",
    "\n",
    "Converts text to vectors for clustering. Takes ~10 min for 5K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 embed.py disaggregated.parquet embedded.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Clustering\n",
    "\n",
    "Groups similar problems together. Takes ~5 min for 5K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python3 cluster.py embedded.parquet clustered.csv --min-cluster-size {MIN_CLUSTER_SIZE} --prob-threshold {PROB_THRESHOLD}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\ndf = pd.read_csv('clustered.csv')\n\nprint(f\"Total rows: {len(df)}\")\nprint(f\"Clusters found: {df[df['cluster'] != -1]['cluster'].nunique()}\")\nprint(f\"Noise/outliers: {(df['cluster'] == -1).sum()}\")\n\nprint(\"\\nFidelity breakdown:\")\nprint(df['fidelity'].value_counts())\n\nprint(\"\\nJourney breakdown (clustered rows only):\")\nprint(df[df['cluster'] != -1]['journey'].value_counts().head(10))\n\nprint(\"\\nTeam breakdown (clustered rows only):\")\nprint(df[df['cluster'] != -1]['team'].value_counts())\n\nprint(\"\\nCluster sizes (top 10):\")\nprint(df[df['cluster'] != -1]['cluster'].value_counts().head(10))\n\nprint(\"\\nSample from largest cluster:\")\nlargest_cluster = df[df['cluster'] != -1]['cluster'].value_counts().index[0]\nsample_df = df[df['cluster'] == largest_cluster][['summarised_problem', 'fidelity', 'journey', 'team']].head(5)\nprint(sample_df.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Push Results to GCS\n\nResults go back to the same bucket. The raw data and outputs never leave GCP."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\n\nfiles_to_push = {\n    \"clustered.csv\": f\"{GCS_OUTPUT}/clustered.csv\",\n    \"disaggregated.parquet\": f\"{GCS_OUTPUT}/disaggregated.parquet\",\n    \"embedded.parquet\": f\"{GCS_OUTPUT}/embedded.parquet\",\n}\n\nfor local_file, gcs_path in files_to_push.items():\n    if os.path.exists(local_file):\n        print(f\"Pushing {local_file} -> {gcs_path}\")\n        result = subprocess.run(\n            [\"gsutil\", \"cp\", local_file, gcs_path],\n            capture_output=True, text=True\n        )\n        if result.returncode != 0:\n            print(f\"  ERROR: {result.stderr}\")\n        else:\n            size_mb = os.path.getsize(local_file) / (1024 * 1024)\n            print(f\"  Done ({size_mb:.1f} MB)\")\n    else:\n        print(f\"Skipping {local_file} (not found)\")\n\nprint(f\"\\nResults in GCS:\")\n!gsutil ls -l {GCS_OUTPUT}/"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## IMPORTANT: Stop Workbench When Done!\n\nGo to **GCP Console -> Vertex AI -> Workbench** and click **STOP** on your instance.\n\nOtherwise it keeps charging ~$8/day."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}